\chapter{Review of Matrix Algebra}
\label{chap:matrix review}

This book assumes the reader has had a course in linear algebra (or has
self-studied it, always the better approach).  This appendix is intended
as a review of basic matrix algebra, or a quick treatment for those
lacking this background.

\section{Terminology and Notation}

A {\bf matrix} is a rectangular array of numbers.  A {\bf vector} is a
matrix with only one row (a {\bf row vector} or only one column (a {\bf
column vector}).

The expression, ``the (i,j) element of a matrix,'' will mean its element
in row i, column j.

Please note the following conventions:

\begin{itemize}

\item Capital letters, e.g. A and X, will be used to denote matrices and
vectors.  

\item Lower-case letters with subscripts, e.g. $a_{2,15}$ and $x_8$,
will be used to denote their elements.

\item Capital letters with subscripts, e.g. $A_{13}$, will be used to
denote submatrices and subvectors.

\end{itemize}

If A is a {\bf square} matrix, i.e. one with equal numbers n of rows and
columns, then its {\bf diagonal} elements are $a_{ii}$, i = 1,...,n.

A square matrix is called {\bf upper-triangular} if $a_{ij} = 0$
whenever $i > j$, with a corresponding definition for {\bf
lower-triangular} matrices.

The {\bf norm} (or {\bf length}) of an n-element vector {\bf X} is 

\begin{equation}
\parallel{X} \parallel = \sqrt{\sum_{i=1}^n x_i^2}
\end{equation}

\subsection{Matrix Addition and Multiplication}

\begin{itemize}

\item For two matrices have the same numbers of rows and same numbers of
columns, addition is defined elementwise, e.g.

\begin{equation}
\left (
\begin{array}{cc}
1 & 5 \\
0 & 3 \\
4 & 8 
\end{array}
\right ) +
\left (
\begin{array}{cc}
6 & 2 \\
0 & 1 \\
4 & 0 
\end{array}
\right ) =
\left (
\begin{array}{cc}
7 & 7 \\
0 & 4 \\
8 & 8 
\end{array}
\right ) 
\end{equation}

\item Multiplication of a matrix by a {\bf scalar}, i.e. a number, is also
defined elementwise, e.g.

\begin{equation}
0.4 \left (
\begin{array}{cc}
7 & 7 \\
0 & 4 \\
8 & 8 
\end{array}
\right ) =
\left (
\begin{array}{cc}
2.8 & 2.8 \\
0 & 1.6 \\
3.2 & 3.2 
\end{array}
\right ) 
\end{equation}

\item The {\bf inner product} or {\bf dot product} of equal-length vectors X
and Y is defined to be

\begin{equation}
\sum_{k=1}^n x_k y_k
\end{equation}

\item The product of matrices A and B is defined if the number of rows
of B equals the number of columns of A (A and B are said to be {\bf
conformable}).  In that case, the (i,j) element of the product C is
defined to be

\begin{equation}
c_{ij} = \sum_{k=1}^n a_{ik} b_{kj}
\end{equation}

For instance,

\begin{equation}
\left (
\begin{array}{cc}
7 & 6 \\
0 & 4 \\
8 & 8 
\end{array}
\right )
\left (
\begin{array}{cc}
1 & 6 \\
2 & 4 
\end{array}
\right ) =
\left (
\begin{array}{cc}
19 & 66 \\
8 & 16 \\
24 & 80
\end{array}
\right ) 
\end{equation}

It is helpful to visualize $c_{ij}$ as the inner product of row i of A
and column j of B, e.g. as shown in bold face here:

\begin{equation}
\left (
\begin{array}{cc}
\mathbf 7 & \mathbf 6 \\
0 & 4 \\
8 & 8 
\end{array}
\right )
\left (
\begin{array}{cc}
\mathbf 1 & 6 \\
\mathbf 2 & 4 
\end{array}
\right ) =
\left (
\begin{array}{cc}
\mathbf 7 & 70 \\
8 & 16 \\
8 & 80
\end{array}
\right ) 
\end{equation}

\item Matrix multiplicatin is associative and distributive, but in
general not commutative:

\begin{equation}
A(BC) = (AB)C
\end{equation}

\begin{equation}
A(B+C) = AB + AC
\end{equation}

\begin{equation}
AB \neq BA
\end{equation}

\end{itemize}

\section{Matrix Transpose}

\begin{itemize}

\item The transpose of a matrix A, denoted $A'$ or $A^{T}$, is obtained by
exchanging the rows and columns of A, e.g.

\begin{equation}
\left (
\begin{array}{cc}
7 & 70 \\
8 & 16 \\
8 & 80
\end{array}
\right )' =
\left (
\begin{array}{ccc}
7 & 8 & 8 \\
70 & 16 & 80
\end{array}
\right )
\end{equation}

\item If A + B is defined, then

\begin{equation}
(A+B)' = A' + B'
\end{equation}

\item If A and B are conformable, then

\begin{equation}
(AB)' = B'A'
\end{equation}

\end{itemize}

\section{Linear Independence}

Equal-length vectors $X_1$,...,$X_k$ are said to be {\bf linearly
independent} if it is impossible for

\begin{equation}
a_1 X_1 +
... +
a_k X_k = 0
\end{equation}

unless all the $a_i$ are 0.

\section{Determinants}

Let A be an nxn matrix.  The definition of the determinant of A,
det(A), involves an abstract formula featuring permutations.  It will be
omitted here, in favor of the following computational method.

Let $A_{-(i,j)}$ denote the submatrix of A obtained by deleting its
i$^{th}$ row and j$^{th}$ column.  Then the determinant can be computed
recursively across the k$^{th}$ row of A as

\begin{equation}
det(A) =
\sum_{m=1}^n (-1)^{k+m} det(A_{-(k,m)})
\end{equation}

where

\begin{equation}
det
\left (
\begin{array}{cc}
s & t   \\
u & v 
\end{array}
\right ) = sv -tu
\end{equation}

Generally, determinants are mainly of theoretical importance, but they
often can clarify one's understanding of concepts.

\section{Matrix Inverse}

\begin{itemize}

\item The {\bf identity} matrix I of size n has 1s in all of its
diagonal elements but 0s in all off-diagonal elements.  It has the
property that AI = A and IA = A whenever those products are defined.

\item The A is a square matrix and AB = I, then B is said to be the
{\bf inverse} of A, denoted $A^{-1}$.  Then BA = I will hold as well.

\item $A^{-1}$ exists if and only if its rows (or columns) are
linearly independent.

\item $A^{-1}$ exists if and only if $det(A) \neq 0$.

\item If A and B are square, conformable and invertible, then AB is also
invertible, and

\begin{equation}
(AB)^{-1} = B^{-1} A^{-1}
\end{equation}

\end{itemize}

A matrix U is said to be {\bf orthogonal} if its rows each have norm 1
and are orthogonal to each other, i.e. their inner product is 0.  U thus
has the property that $U U' = I$ i.e. $U^{-1} = U$.

The inverse of a triangular matrix is easily obtain by something called
{\bf back substitution}.

Typically one does not compute matrix inverses directly.  A common
alternative is the {\bf QR decomposition}:  For a matrix A,
matrices Q and R are calculated so that A = QR, where Q is an orthogonal
matrix and R is upper-triangular.  

If A is square and inveritble, $A^{-1}$ is easily found: 

\begin{equation}
A^{-1} = (QR)^{-1} = R^{-1} Q'
\end{equation}

Again, though, in some cases A is part of a more complex system, and the
inverse is not explicitly computed.

\section{Eigenvalues and Eigenvectors}

Let A be a square matrix.\footnote{For nonsquare matrices, the
discussion here would generalize to the topic of {\bf singular value
decomposition}.}  

\begin{itemize}

\item A scalar $\lambda$ and a nonzero vector X that satisfy

\begin{equation}
AX = \lambda X
\end{equation}

are called an {\bf eigenvalue} and {\bf eigenvector} of A, respectively.

\item If A is symmetric and real, then it is {\bf diagonalizable}, i.e
there exists an orthogonal matrix U such that

\begin{equation}
U'AU = D
\end{equation}

for a diagonal matrix D.  The elements of D are the eigenvalues of A,
and the columns of U are the eigenvectors of A.

\end{itemize}

\section{Matrix Algebra in R}

The R programming language has extensive facilities for matrix algebra,
introduced here.  

Note first that R matrix subscripts, like those of vectors, begin at 1,
rather than 0 as in C/C++.  For instance:

\begin{lstlisting}
> m <- rbind(3:4,c(1,8))
> m
     [,1] [,2]
[1,]    3    4
[2,]    1    8
> m[2,2]
[1] 8
\end{lstlisting}

Next, it is important to know that R uses column-major order, i.e.  its
elements are stored in memory column-by-column.  In the case of the
matrix {\bf m} above, for instance, the element 1 will be the second one
in the internal memory storage of {\bf m}, while the 8 will be the
fourth.  

This is also reflected in how R ``inputs'' data when a matrix is
constructed, e.g.

\begin{lstlisting}
> d <- matrix(c(1,-1,0,0,3,8),nrow=2)
> d
     [,1] [,2] [,3]
[1,]    1    0    3
[2,]   -1    0    8
\end{lstlisting}

The R matrix type is a special case of vectors:

\begin{lstlisting}
> d[5]  # 5th element, i.e. row 1, column 3
[1] 3
\end{lstlisting}

A linear algebra vector can be formed as an R vector, or as a one-row or
one-column matrix.  If you use it in a matrix product, R will usually be
able to figure out whether you mean it to be a row or a column.

\begin{lstlisting}
> # constructing matrices
> a <- rbind(1:3,10:12)
> a
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]   10   11   12
> b <- matrix(1:9,ncol=3)  
> b
     [,1] [,2] [,3]
[1,]    1    4    7
[2,]    2    5    8
[3,]    3    6    9
# multiplication, addition etc.
> c <- a %*% b
> c
     [,1] [,2] [,3]
[1,]   14   32   50
[2,]   68  167  266
> c + matrix(c(1,-1,0,0,3,8),nrow=2)  # 2 different c's!
     [,1] [,2] [,3]
[1,]   15   32   53
[2,]   67  167  274
> c %*% c(1,5,6)  
     [,1]
[1,]  474
[2,] 2499
> t(a)  # matrix transpose
     [,1] [,2]
[1,]    1   10
[2,]    2   11
[3,]    3   12
> # matrix inverse
> u <- matrix(runif(9),nrow=3)
> u
           [,1]       [,2]      [,3]
[1,] 0.08446154 0.86335270 0.6962092
[2,] 0.31174324 0.35352138 0.7310355
[3,] 0.56182226 0.02375487 0.2950227
> uinv <- solve(u)
> uinv
           [,1]      [,2]      [,3]
[1,]  0.5818482 -1.594123  2.576995
[2,]  2.1333965 -2.451237  1.039415
[3,] -1.2798127  3.233115 -1.601586
> u %*% uinv  # check, but note roundoff error
             [,1]          [,2]          [,3]
[1,] 1.000000e+00 -1.680513e-16 -2.283330e-16
[2,] 6.651580e-17  1.000000e+00  4.412703e-17
[3,] 2.287667e-17 -3.539920e-17  1.000000e+00
> # eigenvalues and eigenvectors
> eigen(u)
$values
[1]  1.2456220+0.0000000i -0.2563082+0.2329172i -0.2563082-0.2329172i

$vectors
              [,1]                  [,2]                  [,3]
[1,] -0.6901599+0i -0.6537478+0.0000000i -0.6537478+0.0000000i
[2,] -0.5874584+0i -0.1989163-0.3827132i -0.1989163+0.3827132i
[3,] -0.4225778+0i  0.5666579+0.2558820i  0.5666579-0.2558820i
> # diagonal matrices (off-diagonals 0)
> diag(3)
     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    0    0    1
> diag((c(5,12,13)))
     [,1] [,2] [,3]
[1,]    5    0    0
[2,]    0   12    0
[3,]    0    0   13
> m
     [,1] [,2] [,3]
[1,]    5    6    7
[2,]   10   11   12
> diag(m) <- c(8,88)
> m
     [,1] [,2] [,3]
[1,]    8    6    7
[2,]   10   88   12
\end{lstlisting}
